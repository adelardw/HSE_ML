{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным пользователей социальной сети Вконтакте, и сравнить его эффективность с ансамблем, предоставляемым библиотекой CatBoost.\n",
    "\n",
    "В результате мы сможем определить, какие подписки пользователей больше всего влияют на определение возраста и пола человека. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import mode\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import gini, entropy, gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (2 балла)\n",
    "Random Forest состоит из деревьев решений. Каждое такое дерево строится на одной из выборок, полученных при помощи bagging. Элементы, которые не вошли в новую обучающую выборку, образуют out-of-bag выборку. Кроме того, в каждом узле дерева мы случайным образом выбираем набор из `max_features` и ищем признак для предиката разбиения только в этом наборе.\n",
    "\n",
    "Сегодня мы будем работать только с бинарными признаками, поэтому нет необходимости выбирать значение признака для разбиения.\n",
    "\n",
    "#### Методы\n",
    "`predict(X)` - возвращает предсказанные метки для элементов выборки `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`X, y` - обучающая выборка и соответствующие ей метки классов. Из нее нужно получить выборку для построения дерева при помощи bagging. Out-of-bag выборку нужно запомнить, она понадобится потом.\n",
    "\n",
    "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
    "\n",
    "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
    "\n",
    "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
    "\n",
    "`max_features=\"auto\"` - количество признаков, которые могут использоваться в узле. Если `\"auto\"` - равно `sqrt(X.shape[1])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1,\n",
    "                 max_features=\"auto\", n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in tqdm(range(self.n_estimators)):\n",
    "            tree = DecisionTree(X, y, self.criterion, self.max_depth, self.min_samples_leaf, self.max_features)\n",
    "            #tree.fit()\n",
    "\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = self.trees[0].predict(X)\n",
    "\n",
    "        for tree in self.trees[1:]:\n",
    "            preds = np.vstack((preds, tree.predict(X)))\n",
    "\n",
    "        return mode(preds, axis=0)[0].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла)\n",
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest - посчитать out-of-bag ошибку предсказания `err_oob`, а затем перемешать значения признака `j` и посчитать ее (`err_oob_j`) еще раз. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(rfc):\n",
    "    err_j = np.zeros((len(rfc.trees), rfc.trees[0].oob_X.shape[1]))\n",
    "\n",
    "    for i, tree in enumerate(rfc.trees):\n",
    "        oob_X, oob_y = tree.oob_X, tree.oob_y\n",
    "        N, M = oob_X.shape\n",
    "\n",
    "        err_oob = np.mean(tree.predict(oob_X) == oob_y)        \n",
    "\n",
    "        for j in np.arange(M):\n",
    "            shuffled_X = oob_X.copy()\n",
    "            random_idx = np.random.choice(np.arange(N), N, replace=False)\n",
    "            shuffled_X[:, j] = oob_X[:, j][random_idx]\n",
    "\n",
    "            err_oob_j = np.mean(tree.predict(shuffled_X) == oob_y)\n",
    "            err_j[i][j] = err_oob - err_oob_j\n",
    "\n",
    "    err_j = np.mean(err_j, axis=0)\n",
    "    return err_j\n",
    "\n",
    "def most_important_features(importance, names, k=20):\n",
    "    # Выводит названия k самых важных признаков\n",
    "    idicies = np.argsort(importance)[::-1][:k]\n",
    "    return np.array(names)[idicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, пришло время протестировать наше дерево на простом синтетическом наборе данных. В результате точность должна быть примерно равна `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b6fc49d19149e3bbb4b5a80a615e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yaros\\AppData\\Local\\Temp\\ipykernel_12416\\2887499782.py:25: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return mode(preds, axis=0)[0].squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Importance: [-0.00113 -0.00851  0.20022  0.1808   0.3892  -0.00238]\n"
     ]
    }
   ],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "print(\"Importance:\", feature_importance(rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (3 балла)\n",
    "Теперь поработаем с реальными данными.\n",
    "\n",
    "Выборка состоит из публичных анонимизированных данных пользователей социальной сети Вконтакте. Первые два столбца отражают возрастную группу (`zoomer`, `doomer` и `boomer`) и пол (`female`, `male`). Все остальные столбцы являются бинарными признаками, каждый из них определяет, подписан ли пользователь на определенную группу/публичную страницу или нет.\\\n",
    "\\\n",
    "Необходимо обучить два классификатора, один из которых определяет возрастную группу, а второй - пол.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются. Лес должен строиться за какое-то разумное время.\n",
    "\n",
    "Оценка:\n",
    "1. 1 балл за исправно работающий код\n",
    "2. +1 балл за точность предсказания возростной группы выше 65%\n",
    "3. +1 балл за точность предсказания пола выше 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    dataframe = pandas.read_csv(path, header=0)\n",
    "    dataset = dataframe.values.tolist()\n",
    "    random.shuffle(dataset)\n",
    "    y_age = [row[0] for row in dataset]\n",
    "    y_sex = [row[1] for row in dataset]\n",
    "    X = [row[2:] for row in dataset]\n",
    "    \n",
    "    return np.array(X), np.array(y_age), np.array(y_sex), list(dataframe.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk_train.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f36307895034d6abc845404e0d80dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yaros\\OneDrive\\Рабочий стол\\Homework 09\\task.py:218: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return mode(preds, axis=0)[0].squeeze()\n",
      "c:\\Users\\yaros\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\stats\\_stats_py.py:110: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  warnings.warn(\"The input array could not be properly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7292307692307692\n",
      "Most important features:\n"
     ]
    }
   ],
   "source": [
    "from task import rfc_age\n",
    "\n",
    "rfc_age.fit(X_train, y_age_train)\n",
    "print(\"Accuracy:\", np.mean(rfc_age.predict(X_test) == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc_age), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee94aea8d4a84534a08f8433d5247117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yaros\\AppData\\Local\\Temp\\ipykernel_16848\\3653038425.py:25: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return mode(preds, axis=0)[0].squeeze()\n",
      "C:\\Users\\yaros\\AppData\\Local\\Temp\\ipykernel_16848\\3653038425.py:25: DeprecationWarning: Support for non-numeric arrays has been deprecated as of SciPy 1.9.0 and will be removed in 1.11.0. `pandas.DataFrame.mode` can be used instead, see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html.\n",
      "  return mode(preds, axis=0)[0].squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8769230769230769\n",
      "Most important features:\n",
      "1. mudakoff\n",
      "2. 40kg\n",
      "3. 9o_6o_9o\n",
      "4. rapnewrap\n",
      "5. 4ch\n",
      "6. girlmeme\n",
      "7. academyofman\n",
      "8. igm\n",
      "9. zerofat\n",
      "10. fuck_humor\n",
      "11. bon\n",
      "12. be.women\n",
      "13. be.beauty\n",
      "14. thesmolny\n",
      "15. rhymes\n",
      "16. modnailru\n",
      "17. reflexia_our_feelings\n",
      "18. bot_maxim\n",
      "19. woman.blog\n",
      "20. i_d_t\n"
     ]
    }
   ],
   "source": [
    "from task import rfc_gender\n",
    "\n",
    "rfc_gender = RandomForestClassifier(n_estimators=10)\n",
    "rfc_gender.fit(X_train, y_sex_train)\n",
    "print(\"Accuracy:\", np.mean(rfc_gender.predict(X_test) == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc_gender), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost\n",
    "В качестве аьтернативы попробуем CatBoost. \n",
    "\n",
    "Устаниовить его можно просто с помощью `pip install catboost`. Туториалы можно найти, например, [здесь](https://catboost.ai/docs/concepts/python-usages-examples.html#multiclassification) и [здесь](https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb). Главное - не забудьте использовать `loss_function='MultiClass'`.\\\n",
    "\\\n",
    "Сначала протестируйте CatBoost на синтетических данных. Выведите точность и важность признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0000000\ttotal: 19.8ms\tremaining: 9.88s\n",
      "499:\tlearn: 1.0000000\ttotal: 8.06s\tremaining: 0us\n",
      "Accuracy: 1.0\n",
      "Importance: [ 0.          0.          5.87260854 17.76801882 76.35937264  0.        ]\n"
     ]
    }
   ],
   "source": [
    "X, y = synthetic_dataset(1000)\n",
    "\n",
    "cat_params = {\n",
    "                'n_estimators':500,\n",
    "                'learning_rate': 0.005,\n",
    "                'eval_metric':'Accuracy',\n",
    "                'loss_function':'MultiClass',\n",
    "                'random_seed': 42,\n",
    "                'metric_period':500,\n",
    "                'od_wait':500,\n",
    "                'task_type':'GPU',\n",
    "                'depth': 10,\n",
    "                }\n",
    "                \n",
    "clf = CatBoostClassifier(**cat_params)\n",
    "clf.fit(X, y)\n",
    "\n",
    "y_pred = clf.predict(X).reshape(1, -1)\n",
    "print(\"Accuracy:\", np.mean(y_pred == y))\n",
    "print(\"Importance:\", clf.get_feature_importance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (3 балла)\n",
    "Попробуем применить один из используемых на практике алгоритмов. В этом нам поможет CatBoost. Также, как и реализованный ними RandomForest, применим его для определения пола и возраста пользователей сети Вконтакте, выведите названия наиболее важных признаков так же, как в задании 3.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются.\n",
    "\n",
    "Оценка:\n",
    "1. 1 балл за исправно работающий код\n",
    "2. +1 балл за точность предсказания возростной группы выше 65%\n",
    "3. +1 балл за точность предсказания пола выше 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk_train.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)\n",
    "X_train, X_eval, y_age_train, y_age_eval, y_sex_train, y_sex_eval = train_test_split(X_train, y_age_train, y_sex_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6452991\ttest: 0.6196581\tbest: 0.6196581 (0)\ttotal: 32.8ms\tremaining: 16.4s\n",
      "499:\tlearn: 0.7305556\ttest: 0.7111111\tbest: 0.7111111 (498)\ttotal: 12.3s\tremaining: 0us\n",
      "bestTest = 0.7111111111\n",
      "bestIteration = 498\n",
      "Shrink model to first 499 iterations.\n",
      "Accuracy: 0.6876923076923077\n",
      "Most important features:\n",
      "1. ovsyanochan\n",
      "2. styd.pozor\n",
      "3. 4ch\n",
      "4. rhymes\n",
      "5. tumblr_vacuum\n",
      "6. xfilm\n",
      "7. pixel_stickers\n",
      "8. mudakoff\n",
      "9. leprum\n",
      "10. dayvinchik\n"
     ]
    }
   ],
   "source": [
    "clf = CatBoostClassifier(**cat_params)\n",
    "clf.fit(X_train, y_age_train, eval_set=(X_eval, y_age_eval),\n",
    "            cat_features=np.arange(X.shape[1]),\n",
    "            use_best_model=True,\n",
    "            verbose=True)\n",
    "\n",
    "y_pred = clf.predict(X_test).reshape(1, -1)\n",
    "clf.save_model(\"model_age\")\n",
    "print(\"Accuracy:\", np.mean(y_pred == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(clf.get_feature_importance(), features, 10)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7512821\ttest: 0.7316239\tbest: 0.7316239 (0)\ttotal: 27.6ms\tremaining: 2m 17s\n",
      "500:\tlearn: 0.8623932\ttest: 0.8743590\tbest: 0.8743590 (500)\ttotal: 9.98s\tremaining: 1m 29s\n",
      "1000:\tlearn: 0.8841880\ttest: 0.8820513\tbest: 0.8829060 (977)\ttotal: 20.3s\tremaining: 1m 21s\n",
      "1500:\tlearn: 0.8961538\ttest: 0.8846154\tbest: 0.8854701 (1470)\ttotal: 30.5s\tremaining: 1m 11s\n",
      "2000:\tlearn: 0.9047009\ttest: 0.8871795\tbest: 0.8888889 (1764)\ttotal: 40.9s\tremaining: 1m 1s\n",
      "bestTest = 0.8888888889\n",
      "bestIteration = 1764\n",
      "Shrink model to first 1765 iterations.\n",
      "Accuracy: 0.8553846153846154\n",
      "Most important features:\n",
      "1. 40kg\n",
      "2. mudakoff\n",
      "3. girlmeme\n",
      "4. modnailru\n",
      "5. thesmolny\n",
      "6. 9o_6o_9o\n",
      "7. zerofat\n",
      "8. i_d_t\n",
      "9. reflexia_our_feelings\n",
      "10. academyofman\n"
     ]
    }
   ],
   "source": [
    "clf = CatBoostClassifier(**cat_params)\n",
    "clf.fit(X_train, y_sex_train, eval_set=(X_eval, y_sex_eval),\n",
    "            cat_features=np.arange(X.shape[1]),\n",
    "            use_best_model=True,\n",
    "            verbose=True)\n",
    "\n",
    "y_pred = clf.predict(X_test).reshape(1, -1)\n",
    "clf.save_model(\"model_gender\")\n",
    "print(\"Accuracy:\", np.mean(y_pred == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(clf.get_feature_importance(), features, 10)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import catboost_rfc_age, catboost_rfc_gender\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
